{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ccb1dc",
   "metadata": {},
   "source": [
    "## 1) Quick overview (the goal)\n",
    "\n",
    "You want a model that predicts well on unseen data and is interpretable/stable.\n",
    "\n",
    "Use a mix of: \n",
    "- exploratory (filter) methods, \n",
    "- embedded methods (Lasso, tree importances), and \n",
    "- wrapper methods (RFE/RFECV) \n",
    "— always validated with cross-validation (preferably nested CV when tuning + selecting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b02cd08",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678cd80e",
   "metadata": {},
   "source": [
    "## 2) Step-by-step workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb372e",
   "metadata": {},
   "source": [
    "### 1. Data cleaning & EDA (mandatory)\n",
    "\n",
    "Handle missing values, outliers (only on features if justified), correct datatypes.\n",
    "\n",
    "Visual checks: histograms, boxplots, pairwise scatterplots for numeric features.\n",
    "\n",
    "Treat low frequency category values. Convert categoricals (one-hot, multi-label hot encoding, target-encode where appropriate). Keep cardinality in mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279182b",
   "metadata": {},
   "source": [
    "### 2. Split early (avoid leakage)\n",
    "\n",
    "Split into train / validation / test (e.g. 60/20/20) or use CV. **Do not use test set during selection/tuning.**\n",
    "\n",
    "For reproducibility use random_state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffbce3",
   "metadata": {},
   "source": [
    "### 3. Baseline model\n",
    "\n",
    "Fit a simple baseline (mean predictor, then ordinary OLS / LinearRegression) to get a performance baseline (RMSE, MAE, R²)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df62fd1",
   "metadata": {},
   "source": [
    "### 4. Filter methods (fast, cheap, initial cut)\n",
    "\n",
    "**Correlation matrix**: drop one of any pair with very high correlation (e.g. |corr| > 0.8–0.9) unless both are needed for interpretation.\n",
    "\n",
    "**Univariate tests**: f_regression (ANOVA), mutual information (mutual_info_regression) to rank features.\n",
    "\n",
    "**Business/domain check**: sometimes domain knowledge > statistics — keep features that matter even if stats are weak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa02b96",
   "metadata": {},
   "source": [
    "Example of correlation and mutual info:\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "\n",
    "corr = X.corr().abs()\n",
    "\n",
    "# find pairs\n",
    "high_corr_pairs = [(c1,c2) for c1 in corr.columns for c2 in corr.columns if c1!=c2 and corr.loc[c1,c2]>0.9]\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "f_vals, p_vals = f_regression(X, y)\n",
    "p_series = pd.Series(p_vals, index=X.columns).sort_values()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57250863",
   "metadata": {},
   "source": [
    "### 5. Multicollinearity diagnostics (VIF)\n",
    "\n",
    "Compute VIF; drop or combine features with high VIF (common rules: VIF > 5 or > 10 are suspicious)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082dfd8",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_const = X.assign(const=1)\n",
    "vif = pd.DataFrame({'feature': X_const.columns,\n",
    "                    'VIF': [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]})\n",
    "vif = vif[vif.feature != 'const']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5d2a0",
   "metadata": {},
   "source": [
    "If features are collinear, Ridge often handles prediction better than OLS; Lasso may arbitrarily keep one and drop others (unstable when collinear)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55290414",
   "metadata": {},
   "source": [
    "### 6. Embedded methods (regularization & tree importances)\n",
    "\n",
    "**Lasso (L1)** performs selection: many coefficients → exactly zero.\n",
    "\n",
    "**Ridge (L2)** shrinks coefficients but rarely to zero; use it for multicollinearity / better predictive stability.\n",
    "\n",
    "**ElasticNet mixes L1/L2** — good when correlated predictors + need sparsity.\n",
    "\n",
    "**Tree based models (RandomForest, XGBoost)** give feature importances (useful but not selection-by-default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad5bbc",
   "metadata": {},
   "source": [
    "Lasso model with automatic alpha selection:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('lasso', LassoCV(cv=5, random_state=42))])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Selected features\n",
    "sel = SelectFromModel(pipe.named_steps['lasso'], prefit=True)\n",
    "selected_features = X.columns[sel.get_support()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e39496",
   "metadata": {},
   "source": [
    "Ridge for coeff stability:\n",
    "```python\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), RidgeCV(alphas=[0.01,0.1,1,10], cv=5))\n",
    "ridge.fit(X_train, y_train)\n",
    "coef = ridge.named_steps['ridgecv'].coef_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac3faa",
   "metadata": {},
   "source": [
    "To eliminate with Ridge you can use magnitude thresholds or SelectFromModel(Ridge(...), threshold='median') — but be careful: ridge never gives exact zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c1e9c",
   "metadata": {},
   "source": [
    "### 7. Wrapper methods (RFE / RFECV / sequential)\n",
    "\n",
    "RFE: recursive elimination based on estimator's coefficients/importance.\n",
    "\n",
    "RFECV: RFE + cross-validation chooses the number of features automatically.\n",
    "\n",
    "These are computationally heavier but often yield a compact set that empirically performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f2397",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "est = LinearRegression()\n",
    "rfecv = RFECV(estimator=est, step=1, cv=KFold(5), scoring='neg_mean_squared_error')\n",
    "rfecv.fit(X_train_scaled, y_train)\n",
    "selected = X.columns[rfecv.support_]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c52424",
   "metadata": {},
   "source": [
    "### 8. Nested CV for honest evaluation (important)\n",
    "\n",
    "When doing feature selection and hyperparameter tuning, use nested CV:\n",
    "\n",
    "Outer loop: estimate generalization error.\n",
    "\n",
    "Inner loop: perform tuning (alpha for Lasso/Ridge) and feature selection (SelectFromModel or RFECV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd47dfa",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "outer_cv = KFold(5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectFromModel(LassoCV(cv=inner_cv))),\n",
    "                 ('est', LinearRegression())])\n",
    "\n",
    "# Wrap pipe in GridSearchCV if needed, then cross_val_score on outer_cv\n",
    "scores = cross_val_score(pipe, X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80139f",
   "metadata": {},
   "source": [
    "### 9. Final model & diagnostics\n",
    "\n",
    "After selecting features, retrain on train+val, test on held-out test set.\n",
    "\n",
    "Diagnostics: residual plots, QQ plot (normality), heteroscedasticity (Breusch-Pagan), influence points, partial dependence for non-linear features.\n",
    "\n",
    "If you used statsmodels OLS you can inspect p-values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ef306",
   "metadata": {},
   "source": [
    "```python\n",
    "import statsmodels.api as sm\n",
    "X_sm = sm.add_constant(X_selected)\n",
    "model = sm.OLS(y, X_sm).fit()\n",
    "print(model.summary())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724d034",
   "metadata": {},
   "source": [
    "### 10. Iteration & domain sanity check\n",
    "\n",
    "If performance drops after removing features, reconsider: maybe keep correlated features (Ridge), create interactions, or engineer new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb344d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73da66f",
   "metadata": {},
   "source": [
    "## 3) Practical heuristics & thresholds\n",
    "\n",
    "- Correlation threshold to consider removing one: |corr| > 0.8–0.9 (context dependent).\n",
    "\n",
    "- VIF threshold: VIF > 5 (warning) or > 10 (strong multicollinearity).\n",
    "\n",
    "- p-value (statsmodels): p > 0.05 suggests non-significance — but don’t eliminate blindly; check predictive effect and multicollinearity.\n",
    "\n",
    "- Lasso: non-zero coefficients → keep; zero → candidate to drop.\n",
    "\n",
    "- RFECV: choose the number of features where CV error is minimal (or within 1-SE rule).\n",
    "\n",
    "- If many correlated features: prefer Ridge or ElasticNet over Lasso alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c7e01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9383b3d",
   "metadata": {},
   "source": [
    "## 4) Pitfalls to avoid\n",
    "\n",
    "- Data leakage: Do feature selection inside CV folds, not on full dataset before CV.\n",
    "\n",
    "- Scaling: Regularization needs features scaled (StandardScaler) — do scaling in a Pipeline.\n",
    "\n",
    "- Too few samples: heavy feature selection can overfit when n_samples ≈ n_features. Use regularization and/or dimensionality reduction (PCA) with caution.\n",
    "\n",
    "- Unstable Lasso selection: with correlated predictors Lasso may jump — use ElasticNet or stability selection (bootstrap + Lasso) to find robust features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b38b62",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba38100",
   "metadata": {},
   "source": [
    "## 5) Short checklist (actionable)\n",
    "\n",
    "- Clean data, \n",
    "\n",
    "- EDA: distributions, correlations, domain checks.\n",
    "\n",
    "- split (train/val/test).\n",
    "\n",
    "- Baseline OLS model → metric.\n",
    "\n",
    "- Remove trivially bad features (missingness, zero variance).\n",
    "\n",
    "- Run filter methods (corr, MI, f_regression) — drop obvious redundancies.\n",
    "\n",
    "- Check VIF; resolve collinearity.\n",
    "\n",
    "- Use embedded (LassoCV, RidgeCV, ElasticNetCV) inside a Pipeline (StandardScaler first).\n",
    "\n",
    "- Optionally run RFECV to confirm compact subset.\n",
    "\n",
    "- Evaluate with nested CV if you tune + select.\n",
    "\n",
    "- Retrain on full train+val; test on holdout; run diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e42e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fe4d0",
   "metadata": {},
   "source": [
    "## 6) Minimal recommended code template (put together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527581e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Example pipeline with Lasso selection then LinearRegression refit\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectFromModel(LassoCV(cv=5, random_state=42))),\n",
    "    ('est', LinearRegression())\n",
    "])\n",
    "\n",
    "cv = KFold(5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(pipe, X, y, cv=cv, scoring='neg_mean_squared_error')\n",
    "print(\"CV RMSE\", (-scores.mean())**0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ecb52",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e63fe3",
   "metadata": {},
   "source": [
    "## 7) Full notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📘 Feature Selection Notebook for Linear, Ridge, and Lasso Regression\n",
    "# ===============================================================\n",
    "\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Replace target column name with yours\n",
    "target = \"price\"  \n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf750c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Exploratory Data Analysis (EDA)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nBasic Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\nSummary stats:\")\n",
    "display(df.describe(include=\"all\"))\n",
    "\n",
    "# Correlation heatmap (numeric features only)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=False, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Target distribution\n",
    "sns.histplot(df[target], kde=True)\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. Handle Categorical Variables\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Separate numerical & categorical\n",
    "num_cols = df.select_dtypes(include=np.number).columns.drop(target)\n",
    "cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "print(\"Numeric columns:\", list(num_cols))\n",
    "print(\"Categorical columns:\", list(cat_cols))\n",
    "\n",
    "# --- One-hot encoding (for nominal categories) ---\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# If you have *multi-label categories* (e.g. a column with \"A,B,C\"),\n",
    "# split them into sets and binarize:\n",
    "# Example: df['genres'] = \"Action,Drama\"\n",
    "if 'genres' in df.columns:  # replace with your multi-label column name\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df = df.join(pd.DataFrame(mlb.fit_transform(df['genres'].str.split(',')),\n",
    "                              columns=mlb.classes_,\n",
    "                              index=df.index))\n",
    "    df = df.drop(columns=['genres'])\n",
    "\n",
    "print(\"Encoded dataset shape:\", df_encoded.shape)\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Variance Inflation Factor (VIF) for Multicollinearity\n",
    "X_vif = X_train.copy()\n",
    "X_vif_const = sm.add_constant(X_vif)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X_vif_const.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_vif_const.values, i)\n",
    "              for i in range(X_vif_const.shape[1])]\n",
    "\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Baseline Linear Regression\n",
    "lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Baseline R²:\", r2_score(y_test, y_pred))\n",
    "print(\"Baseline RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a. LassoCV for Feature Selection\n",
    "lasso = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LassoCV(cv=5, random_state=42))\n",
    "])\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"Chosen alpha:\", lasso.named_steps['model'].alpha_)\n",
    "coef = pd.Series(lasso.named_steps['model'].coef_, index=X.columns)\n",
    "print(\"Selected features:\", list(coef[coef != 0].index))\n",
    "\n",
    "# Plot coefficients\n",
    "plt.figure(figsize=(10,6))\n",
    "coef.plot(kind='bar')\n",
    "plt.title(\"Lasso Coefficients\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b. RidgeCV for Coefficient Shrinkage\n",
    "ridge = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5))\n",
    "])\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Chosen alpha (Ridge):\", ridge.named_steps['model'].alpha_)\n",
    "coef_ridge = pd.Series(ridge.named_steps['model'].coef_, index=X.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c. Compare Ridge vs Lasso vs Linear\n",
    "lr_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "coef_lr = pd.Series(lr_model.named_steps['model'].coef_, index=X.columns)\n",
    "coef_lasso = pd.Series(lasso.named_steps['model'].coef_, index=X.columns)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Linear\": coef_lr,\n",
    "    \"Ridge\": coef_ridge,\n",
    "    \"Lasso\": coef_lasso\n",
    "})\n",
    "\n",
    "coef_df.plot(kind=\"bar\", figsize=(12,6))\n",
    "plt.title(\"Coefficient Comparison: Linear vs Ridge vs Lasso\")\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06230fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. RFECV with Linear Regression\n",
    "rfecv = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', RFECV(estimator=LinearRegression(), step=1, cv=5,\n",
    "                       scoring='neg_mean_squared_error'))\n",
    "])\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal number of features:\", rfecv.named_steps['selector'].n_features_)\n",
    "selected_features = X.columns[rfecv.named_steps['selector'].support_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "plt.plot(rfecv.named_steps['selector'].grid_scores_)\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"CV Score\")\n",
    "plt.title(\"RFECV Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Nested CV with Lasso for Honest Evaluation\n",
    "outer_cv = KFold(5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(3, shuffle=True, random_state=1)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectFromModel(LassoCV(cv=inner_cv, random_state=1))),\n",
    "    ('est', LinearRegression())\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n",
    "print(\"Nested CV RMSE:\", np.mean(np.sqrt(-scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75088ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Final Diagnostics (Residual Plots)\n",
    "final_model = lasso.fit(X_train, y_train)\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "residuals = y_test - y_pred_final\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=y_pred_final, y=residuals)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821efc3",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975295a",
   "metadata": {},
   "source": [
    "## Do you encode before or after train-test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6fe9d",
   "metadata": {},
   "source": [
    "**Option 1: Encode before train–test split**\n",
    "\n",
    "- You run pd.get_dummies() or LabelEncoder on the full dataset first, then split.\n",
    "\n",
    "- Problem: You risk data leakage.\n",
    "- Example: Imagine a categorical variable \"city\" with 100 categories. If some cities only appear in the test set, your training encoding will never learn them → mismatch in columns.\n",
    "\n",
    "- Or, if you encode before splitting, the test set “influences” the feature space (because you looked at all categories). That leaks information about the test distribution.\n",
    "\n",
    "**Option 2: Encode inside a pipeline**\n",
    "\n",
    "- Use OneHotEncoder (or similar) as a pipeline step after splitting.\n",
    "\n",
    "- This way, the encoder is fit only on the training data.\n",
    "\n",
    "- When applied to the test data, it ignores unseen categories (or handles them if you set handle_unknown=\"ignore\").\n",
    "\n",
    "- This avoids leakage and keeps the workflow reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee44e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with ColumnTransformer and Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Identify column types\n",
    "num_cols = X.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = X.select_dtypes(exclude=['int64','float64']).columns\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('regressor', LassoCV(cv=5, random_state=42))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R²:\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1c32a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Load Data\n",
    "df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "target = \"price\"   # <-- replace with your target column\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Split before preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Identify numeric & categorical columns\n",
    "\n",
    "# Identify column types\n",
    "num_cols = X_train.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = X_train.select_dtypes(exclude=['int64','float64']).columns\n",
    "\n",
    "print(\"Numeric columns:\", list(num_cols))\n",
    "print(\"Categorical columns:\", list(cat_cols)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocessor with ColumnTransformer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Pipelines for each model\n",
    "\n",
    "# Linear Regression\n",
    "lr_pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# LassoCV\n",
    "lasso_pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', LassoCV(cv=5, random_state=42))\n",
    "])\n",
    "\n",
    "# RidgeCV\n",
    "ridge_pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Fit and compare models\n",
    "models = {\n",
    "    \"Linear\": lr_pipe,\n",
    "    \"Lasso\": lasso_pipe,\n",
    "    \"Ridge\": ridge_pipe\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    results[name] = {\n",
    "        \"R2\": r2_score(y_test, y_pred),\n",
    "        \"RMSE\": mean_squared_error(y_test, y_pred, squared=False)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Extract coefficients after pipeline\n",
    "\n",
    "# One subtlety: after encoding, feature names expand. You can recover them:\n",
    "\n",
    "# Example: get feature names from Lasso\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "ohe = lasso_pipe.named_steps['preprocess'].named_transformers_['cat']\n",
    "encoded_cat_cols = ohe.get_feature_names_out(cat_cols)\n",
    "all_features = np.concatenate([num_cols, encoded_cat_cols])\n",
    "\n",
    "coef = pd.Series(lasso_pipe.named_steps['model'].coef_, index=all_features)\n",
    "\n",
    "print(\"Selected features:\", list(coef[coef != 0].index))\n",
    "coef.sort_values().plot(kind=\"bar\", figsize=(12,6))\n",
    "plt.title(\"Lasso Coefficients\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b7421",
   "metadata": {},
   "source": [
    "✅ Benefits of this version\n",
    "\n",
    "- Safe: no leakage (encoding/scaling fit only on train).\n",
    "\n",
    "- Flexible: handles unknown categories in test data.\n",
    "\n",
    "- Clean: same pipeline can be saved and reused (joblib.dump / joblib.load).\n",
    "\n",
    "- Transparent: easy to extract feature names and coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0f21d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac642433",
   "metadata": {},
   "source": [
    "## Saving and reusing the pipeline\n",
    "\n",
    "When we train a pipeline, it’s not just the model (like Ridge or Lasso) that gets fitted — the whole pipeline stores the preprocessing steps too (imputers, encoders, scalers, feature selectors, etc.).\n",
    "\n",
    "That’s powerful, because:\n",
    "\n",
    "You don’t need to repeat one-hot encoding, scaling, or VIF checks separately later.\n",
    "\n",
    "The pipeline ensures that new/unseen data goes through exactly the same transformations as training data.\n",
    "\n",
    "You can save the whole pipeline to disk with joblib.dump and reload it for predictions with joblib.load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(model, \"ridge_pipeline.pkl\")\n",
    "\n",
    "# Later / elsewhere\n",
    "loaded_model = joblib.load(\"ridge_pipeline.pkl\")\n",
    "y_pred = loaded_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c63133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, Ridge\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Example dataset (replace with your own)\n",
    "X = pd.DataFrame({\n",
    "    \"age\": [23, 45, 31, 35, 40, 50, 29, 33],\n",
    "    \"salary\": [40000, 80000, 50000, 60000, 70000, 120000, 45000, 55000],\n",
    "    \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"C\", \"A\", \"C\"]\n",
    "})\n",
    "y = [200, 400, 250, 300, 350, 600, 220, 280]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "numeric_features = [\"age\", \"salary\"]\n",
    "categorical_features = [\"city\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Feature selector + model\n",
    "feature_selector = RFECV(estimator=LinearRegression(), cv=3, scoring=\"r2\")\n",
    "\n",
    "# Final pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", feature_selector),\n",
    "    (\"regressor\", LassoCV(cv=5, random_state=42))  # Could switch to Ridge, LinearRegression, etc.\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Save entire pipeline\n",
    "joblib.dump(pipeline, \"regression_pipeline.pkl\")\n",
    "\n",
    "# Load later\n",
    "loaded_pipeline = joblib.load(\"regression_pipeline.pkl\")\n",
    "print(\"Predictions (loaded):\", loaded_pipeline.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c7806",
   "metadata": {},
   "source": [
    "👉 So instead of saving just the model (which would forget how to encode or scale inputs), you save the pipeline, which knows how to:\n",
    "\n",
    "- Impute missing values\n",
    "- Encode categories\n",
    "- Scale numerics\n",
    "- Apply regression\n",
    "\n",
    "That makes it clean, consistent, and production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e7c37",
   "metadata": {},
   "source": [
    "----\n",
    "| Feature                  | GridSearchCV                           | RidgeCV / LassoCV                      |\n",
    "|---------------------------|-----------------------------------------|----------------------------------------|\n",
    "| **Applies to**           | Any model in scikit-learn               | Only Ridge or Lasso regression         |\n",
    "| **Hyperparameters**      | Any (e.g., alpha, max_iter, solver, …)  | Only alpha (regularization strength)   |\n",
    "| **Cross-validation**     | Yes (user-specified folds)              | Yes (built-in, default = 5 folds)      |\n",
    "| **Efficiency**           | Brute force search, can be slow         | Optimized solvers, very fast           |\n",
    "| **Ease of use**          | Flexible but more boilerplate code      | Very simple, just pass `alphas`        |\n",
    "| **Output**               | Best estimator + CV scores              | Best alpha + fitted model              |\n",
    "| **When to use**          | General hyperparameter tuning           | Regularization tuning (Ridge/Lasso)    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec6e0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
