{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ccb1dc",
   "metadata": {},
   "source": [
    "## 1) Quick overview (the goal)\n",
    "\n",
    "You want a model that predicts well on unseen data and is interpretable/stable.\n",
    "\n",
    "Use a mix of: \n",
    "- exploratory (filter) methods, \n",
    "- embedded methods (Lasso, tree importances), and \n",
    "- wrapper methods (RFE/RFECV) \n",
    "‚Äî always validated with cross-validation (preferably nested CV when tuning + selecting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b02cd08",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678cd80e",
   "metadata": {},
   "source": [
    "## 2) Step-by-step workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb372e",
   "metadata": {},
   "source": [
    "### 1. Data cleaning & EDA (mandatory)\n",
    "\n",
    "Handle missing values, outliers (only on features if justified), correct datatypes.\n",
    "\n",
    "Visual checks: histograms, boxplots, pairwise scatterplots for numeric features.\n",
    "\n",
    "Treat low frequency category values. Convert categoricals (one-hot, multi-label hot encoding, target-encode where appropriate). Keep cardinality in mind. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279182b",
   "metadata": {},
   "source": [
    "### 2. Split early (avoid leakage)\n",
    "\n",
    "Split into train / validation / test (e.g. 60/20/20) or use CV. **Do not use test set during selection/tuning.**\n",
    "\n",
    "For reproducibility use random_state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ffbce3",
   "metadata": {},
   "source": [
    "### 3. Baseline model\n",
    "\n",
    "Fit a simple baseline (mean predictor, then ordinary OLS / LinearRegression) to get a performance baseline (RMSE, MAE, R¬≤)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df62fd1",
   "metadata": {},
   "source": [
    "### 4. Filter methods (fast, cheap, initial cut)\n",
    "\n",
    "**Correlation matrix**: drop one of any pair with very high correlation (e.g. |corr| > 0.8‚Äì0.9) unless both are needed for interpretation.\n",
    "\n",
    "**Univariate tests**: f_regression (ANOVA), mutual information (mutual_info_regression) to rank features.\n",
    "\n",
    "**Business/domain check**: sometimes domain knowledge > statistics ‚Äî keep features that matter even if stats are weak."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa02b96",
   "metadata": {},
   "source": [
    "Example of correlation and mutual info:\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "\n",
    "corr = X.corr().abs()\n",
    "\n",
    "# find pairs\n",
    "high_corr_pairs = [(c1,c2) for c1 in corr.columns for c2 in corr.columns if c1!=c2 and corr.loc[c1,c2]>0.9]\n",
    "\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi_series = pd.Series(mi, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "f_vals, p_vals = f_regression(X, y)\n",
    "p_series = pd.Series(p_vals, index=X.columns).sort_values()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57250863",
   "metadata": {},
   "source": [
    "### 5. Multicollinearity diagnostics (VIF)\n",
    "\n",
    "Compute VIF; drop or combine features with high VIF (common rules: VIF > 5 or > 10 are suspicious)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082dfd8",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "X_const = X.assign(const=1)\n",
    "vif = pd.DataFrame({'feature': X_const.columns,\n",
    "                    'VIF': [variance_inflation_factor(X_const.values, i) for i in range(X_const.shape[1])]})\n",
    "vif = vif[vif.feature != 'const']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5d2a0",
   "metadata": {},
   "source": [
    "If features are collinear, Ridge often handles prediction better than OLS; Lasso may arbitrarily keep one and drop others (unstable when collinear)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55290414",
   "metadata": {},
   "source": [
    "### 6. Embedded methods (regularization & tree importances)\n",
    "\n",
    "**Lasso (L1)** performs selection: many coefficients ‚Üí exactly zero.\n",
    "\n",
    "**Ridge (L2)** shrinks coefficients but rarely to zero; use it for multicollinearity / better predictive stability.\n",
    "\n",
    "**ElasticNet mixes L1/L2** ‚Äî good when correlated predictors + need sparsity.\n",
    "\n",
    "**Tree based models (RandomForest, XGBoost)** give feature importances (useful but not selection-by-default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad5bbc",
   "metadata": {},
   "source": [
    "Lasso model with automatic alpha selection:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('lasso', LassoCV(cv=5, random_state=42))])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Selected features\n",
    "sel = SelectFromModel(pipe.named_steps['lasso'], prefit=True)\n",
    "selected_features = X.columns[sel.get_support()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e39496",
   "metadata": {},
   "source": [
    "Ridge for coeff stability:\n",
    "```python\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "ridge = make_pipeline(StandardScaler(), RidgeCV(alphas=[0.01,0.1,1,10], cv=5))\n",
    "ridge.fit(X_train, y_train)\n",
    "coef = ridge.named_steps['ridgecv'].coef_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac3faa",
   "metadata": {},
   "source": [
    "To eliminate with Ridge you can use magnitude thresholds or SelectFromModel(Ridge(...), threshold='median') ‚Äî but be careful: ridge never gives exact zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073c1e9c",
   "metadata": {},
   "source": [
    "### 7. Wrapper methods (RFE / RFECV / sequential)\n",
    "\n",
    "RFE: recursive elimination based on estimator's coefficients/importance.\n",
    "\n",
    "RFECV: RFE + cross-validation chooses the number of features automatically.\n",
    "\n",
    "These are computationally heavier but often yield a compact set that empirically performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f2397",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "est = LinearRegression()\n",
    "rfecv = RFECV(estimator=est, step=1, cv=KFold(5), scoring='neg_mean_squared_error')\n",
    "rfecv.fit(X_train_scaled, y_train)\n",
    "selected = X.columns[rfecv.support_]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c52424",
   "metadata": {},
   "source": [
    "### 8. Nested CV for honest evaluation (important)\n",
    "\n",
    "When doing feature selection and hyperparameter tuning, use nested CV:\n",
    "\n",
    "Outer loop: estimate generalization error.\n",
    "\n",
    "Inner loop: perform tuning (alpha for Lasso/Ridge) and feature selection (SelectFromModel or RFECV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd47dfa",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "outer_cv = KFold(5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('selector', SelectFromModel(LassoCV(cv=inner_cv))),\n",
    "                 ('est', LinearRegression())])\n",
    "\n",
    "# Wrap pipe in GridSearchCV if needed, then cross_val_score on outer_cv\n",
    "scores = cross_val_score(pipe, X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80139f",
   "metadata": {},
   "source": [
    "### 9. Final model & diagnostics\n",
    "\n",
    "After selecting features, retrain on train+val, test on held-out test set.\n",
    "\n",
    "Diagnostics: residual plots, QQ plot (normality), heteroscedasticity (Breusch-Pagan), influence points, partial dependence for non-linear features.\n",
    "\n",
    "If you used statsmodels OLS you can inspect p-values:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891ef306",
   "metadata": {},
   "source": [
    "```python\n",
    "import statsmodels.api as sm\n",
    "X_sm = sm.add_constant(X_selected)\n",
    "model = sm.OLS(y, X_sm).fit()\n",
    "print(model.summary())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724d034",
   "metadata": {},
   "source": [
    "### 10. Iteration & domain sanity check\n",
    "\n",
    "If performance drops after removing features, reconsider: maybe keep correlated features (Ridge), create interactions, or engineer new features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cb344d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73da66f",
   "metadata": {},
   "source": [
    "## 3) Practical heuristics & thresholds\n",
    "\n",
    "- Correlation threshold to consider removing one: |corr| > 0.8‚Äì0.9 (context dependent).\n",
    "\n",
    "- VIF threshold: VIF > 5 (warning) or > 10 (strong multicollinearity).\n",
    "\n",
    "- p-value (statsmodels): p > 0.05 suggests non-significance ‚Äî but don‚Äôt eliminate blindly; check predictive effect and multicollinearity.\n",
    "\n",
    "- Lasso: non-zero coefficients ‚Üí keep; zero ‚Üí candidate to drop.\n",
    "\n",
    "- RFECV: choose the number of features where CV error is minimal (or within 1-SE rule).\n",
    "\n",
    "- If many correlated features: prefer Ridge or ElasticNet over Lasso alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98c7e01",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9383b3d",
   "metadata": {},
   "source": [
    "## 4) Pitfalls to avoid\n",
    "\n",
    "- Data leakage: Do feature selection inside CV folds, not on full dataset before CV.\n",
    "\n",
    "- Scaling: Regularization needs features scaled (StandardScaler) ‚Äî do scaling in a Pipeline.\n",
    "\n",
    "- Too few samples: heavy feature selection can overfit when n_samples ‚âà n_features. Use regularization and/or dimensionality reduction (PCA) with caution.\n",
    "\n",
    "- Unstable Lasso selection: with correlated predictors Lasso may jump ‚Äî use ElasticNet or stability selection (bootstrap + Lasso) to find robust features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b38b62",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba38100",
   "metadata": {},
   "source": [
    "## 5) Short checklist (actionable)\n",
    "\n",
    "- Clean data, \n",
    "\n",
    "- EDA: distributions, correlations, domain checks.\n",
    "\n",
    "- split (train/val/test).\n",
    "\n",
    "- Baseline OLS model ‚Üí metric.\n",
    "\n",
    "- Remove trivially bad features (missingness, zero variance).\n",
    "\n",
    "- Run filter methods (corr, MI, f_regression) ‚Äî drop obvious redundancies.\n",
    "\n",
    "- Check VIF; resolve collinearity.\n",
    "\n",
    "- Use embedded (LassoCV, RidgeCV, ElasticNetCV) inside a Pipeline (StandardScaler first).\n",
    "\n",
    "- Optionally run RFECV to confirm compact subset.\n",
    "\n",
    "- Evaluate with nested CV if you tune + select.\n",
    "\n",
    "- Retrain on full train+val; test on holdout; run diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8e42e3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8fe4d0",
   "metadata": {},
   "source": [
    "## 6) Minimal recommended code template (put together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527581e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV, RidgeCV, LinearRegression\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Example pipeline with Lasso selection then LinearRegression refit\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectFromModel(LassoCV(cv=5, random_state=42))),\n",
    "    ('est', LinearRegression())\n",
    "])\n",
    "\n",
    "cv = KFold(5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(pipe, X, y, cv=cv, scoring='neg_mean_squared_error')\n",
    "print(\"CV RMSE\", (-scores.mean())**0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707ecb52",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e63fe3",
   "metadata": {},
   "source": [
    "## 7) Full notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b7552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìò Feature Selection Notebook for Linear, Ridge, and Lasso Regression\n",
    "# ===============================================================\n",
    "\n",
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Replace target column name with yours\n",
    "target = \"price\"  \n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf750c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Exploratory Data Analysis (EDA)\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nBasic Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\nSummary stats:\")\n",
    "display(df.describe(include=\"all\"))\n",
    "\n",
    "# Correlation heatmap (numeric features only)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=False, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Target distribution\n",
    "sns.histplot(df[target], kde=True)\n",
    "plt.title(\"Target Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63cd570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. Handle Categorical Variables\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Separate numerical & categorical\n",
    "num_cols = df.select_dtypes(include=np.number).columns.drop(target)\n",
    "cat_cols = df.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "print(\"Numeric columns:\", list(num_cols))\n",
    "print(\"Categorical columns:\", list(cat_cols))\n",
    "\n",
    "# --- One-hot encoding (for nominal categories) ---\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# If you have *multi-label categories* (e.g. a column with \"A,B,C\"),\n",
    "# split them into sets and binarize:\n",
    "# Example: df['genres'] = \"Action,Drama\"\n",
    "if 'genres' in df.columns:  # replace with your multi-label column name\n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    df = df.join(pd.DataFrame(mlb.fit_transform(df['genres'].str.split(',')),\n",
    "                              columns=mlb.classes_,\n",
    "                              index=df.index))\n",
    "    df = df.drop(columns=['genres'])\n",
    "\n",
    "print(\"Encoded dataset shape:\", df_encoded.shape)\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab0a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f122f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Variance Inflation Factor (VIF) for Multicollinearity\n",
    "X_vif = X_train.copy()\n",
    "X_vif_const = sm.add_constant(X_vif)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X_vif_const.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X_vif_const.values, i)\n",
    "              for i in range(X_vif_const.shape[1])]\n",
    "\n",
    "print(vif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Baseline Linear Regression\n",
    "lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"Baseline R¬≤:\", r2_score(y_test, y_pred))\n",
    "print(\"Baseline RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a. LassoCV for Feature Selection\n",
    "lasso = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LassoCV(cv=5, random_state=42))\n",
    "])\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "print(\"Chosen alpha:\", lasso.named_steps['model'].alpha_)\n",
    "coef = pd.Series(lasso.named_steps['model'].coef_, index=X.columns)\n",
    "print(\"Selected features:\", list(coef[coef != 0].index))\n",
    "\n",
    "# Plot coefficients\n",
    "plt.figure(figsize=(10,6))\n",
    "coef.plot(kind='bar')\n",
    "plt.title(\"Lasso Coefficients\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdf8a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b. RidgeCV for Coefficient Shrinkage\n",
    "ridge = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5))\n",
    "])\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "print(\"Chosen alpha (Ridge):\", ridge.named_steps['model'].alpha_)\n",
    "coef_ridge = pd.Series(ridge.named_steps['model'].coef_, index=X.columns)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6c. Compare Ridge vs Lasso vs Linear\n",
    "lr_model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LinearRegression())\n",
    "]).fit(X_train, y_train)\n",
    "\n",
    "coef_lr = pd.Series(lr_model.named_steps['model'].coef_, index=X.columns)\n",
    "coef_lasso = pd.Series(lasso.named_steps['model'].coef_, index=X.columns)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Linear\": coef_lr,\n",
    "    \"Ridge\": coef_ridge,\n",
    "    \"Lasso\": coef_lasso\n",
    "})\n",
    "\n",
    "coef_df.plot(kind=\"bar\", figsize=(12,6))\n",
    "plt.title(\"Coefficient Comparison: Linear vs Ridge vs Lasso\")\n",
    "plt.axhline(0, color=\"black\", linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06230fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. RFECV with Linear Regression\n",
    "rfecv = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', RFECV(estimator=LinearRegression(), step=1, cv=5,\n",
    "                       scoring='neg_mean_squared_error'))\n",
    "])\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal number of features:\", rfecv.named_steps['selector'].n_features_)\n",
    "selected_features = X.columns[rfecv.named_steps['selector'].support_]\n",
    "print(\"Selected features:\", selected_features)\n",
    "\n",
    "plt.plot(rfecv.named_steps['selector'].grid_scores_)\n",
    "plt.xlabel(\"Number of Features\")\n",
    "plt.ylabel(\"CV Score\")\n",
    "plt.title(\"RFECV Performance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Nested CV with Lasso for Honest Evaluation\n",
    "outer_cv = KFold(5, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(3, shuffle=True, random_state=1)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectFromModel(LassoCV(cv=inner_cv, random_state=1))),\n",
    "    ('est', LinearRegression())\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipe, X, y, cv=outer_cv, scoring='neg_mean_squared_error')\n",
    "print(\"Nested CV RMSE:\", np.mean(np.sqrt(-scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75088ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Final Diagnostics (Residual Plots)\n",
    "final_model = lasso.fit(X_train, y_train)\n",
    "y_pred_final = final_model.predict(X_test)\n",
    "\n",
    "residuals = y_test - y_pred_final\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=y_pred_final, y=residuals)\n",
    "plt.axhline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821efc3",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c975295a",
   "metadata": {},
   "source": [
    "## Do you encode before or after train-test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6fe9d",
   "metadata": {},
   "source": [
    "**Option 1: Encode before train‚Äìtest split**\n",
    "\n",
    "- You run pd.get_dummies() or LabelEncoder on the full dataset first, then split.\n",
    "\n",
    "- Problem: You risk data leakage.\n",
    "- Example: Imagine a categorical variable \"city\" with 100 categories. If some cities only appear in the test set, your training encoding will never learn them ‚Üí mismatch in columns.\n",
    "\n",
    "- Or, if you encode before splitting, the test set ‚Äúinfluences‚Äù the feature space (because you looked at all categories). That leaks information about the test distribution.\n",
    "\n",
    "**Option 2: Encode inside a pipeline**\n",
    "\n",
    "- Use OneHotEncoder (or similar) as a pipeline step after splitting.\n",
    "\n",
    "- This way, the encoder is fit only on the training data.\n",
    "\n",
    "- When applied to the test data, it ignores unseen categories (or handles them if you set handle_unknown=\"ignore\").\n",
    "\n",
    "- This avoids leakage and keeps the workflow reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee44e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with ColumnTransformer and Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Identify column types\n",
    "num_cols = X.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = X.select_dtypes(exclude=['int64','float64']).columns\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocess', preprocessor),\n",
    "    ('regressor', LassoCV(cv=5, random_state=42))\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"R¬≤:\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a1c32a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee75b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Load Data\n",
    "df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "target = \"price\"   # <-- replace with your target column\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# Split before preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Identify numeric & categorical columns\n",
    "\n",
    "# Identify column types\n",
    "num_cols = X_train.select_dtypes(include=['int64','float64']).columns\n",
    "cat_cols = X_train.select_dtypes(exclude=['int64','float64']).columns\n",
    "\n",
    "print(\"Numeric columns:\", list(num_cols))\n",
    "print(\"Categorical columns:\", list(cat_cols)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocessor with ColumnTransformer\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Define preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Pipelines for each model\n",
    "\n",
    "# Linear Regression\n",
    "lr_pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# LassoCV\n",
    "lasso_pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', LassoCV(cv=5, random_state=42))\n",
    "])\n",
    "\n",
    "# RidgeCV\n",
    "ridge_pipe = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('model', RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Fit and compare models\n",
    "models = {\n",
    "    \"Linear\": lr_pipe,\n",
    "    \"Lasso\": lasso_pipe,\n",
    "    \"Ridge\": ridge_pipe\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    results[name] = {\n",
    "        \"R2\": r2_score(y_test, y_pred),\n",
    "        \"RMSE\": mean_squared_error(y_test, y_pred, squared=False)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ea6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Extract coefficients after pipeline\n",
    "\n",
    "# One subtlety: after encoding, feature names expand. You can recover them:\n",
    "\n",
    "# Example: get feature names from Lasso\n",
    "lasso_pipe.fit(X_train, y_train)\n",
    "\n",
    "ohe = lasso_pipe.named_steps['preprocess'].named_transformers_['cat']\n",
    "encoded_cat_cols = ohe.get_feature_names_out(cat_cols)\n",
    "all_features = np.concatenate([num_cols, encoded_cat_cols])\n",
    "\n",
    "coef = pd.Series(lasso_pipe.named_steps['model'].coef_, index=all_features)\n",
    "\n",
    "print(\"Selected features:\", list(coef[coef != 0].index))\n",
    "coef.sort_values().plot(kind=\"bar\", figsize=(12,6))\n",
    "plt.title(\"Lasso Coefficients\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b7421",
   "metadata": {},
   "source": [
    "‚úÖ Benefits of this version\n",
    "\n",
    "- Safe: no leakage (encoding/scaling fit only on train).\n",
    "\n",
    "- Flexible: handles unknown categories in test data.\n",
    "\n",
    "- Clean: same pipeline can be saved and reused (joblib.dump / joblib.load).\n",
    "\n",
    "- Transparent: easy to extract feature names and coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0f21d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac642433",
   "metadata": {},
   "source": [
    "## Saving and reusing the pipeline\n",
    "\n",
    "When we train a pipeline, it‚Äôs not just the model (like Ridge or Lasso) that gets fitted ‚Äî the whole pipeline stores the preprocessing steps too (imputers, encoders, scalers, feature selectors, etc.).\n",
    "\n",
    "That‚Äôs powerful, because:\n",
    "\n",
    "You don‚Äôt need to repeat one-hot encoding, scaling, or VIF checks separately later.\n",
    "\n",
    "The pipeline ensures that new/unseen data goes through exactly the same transformations as training data.\n",
    "\n",
    "You can save the whole pipeline to disk with joblib.dump and reload it for predictions with joblib.load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2f7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessor\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Full pipeline\n",
    "model = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save pipeline\n",
    "joblib.dump(model, \"ridge_pipeline.pkl\")\n",
    "\n",
    "# Later / elsewhere\n",
    "loaded_model = joblib.load(\"ridge_pipeline.pkl\")\n",
    "y_pred = loaded_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c63133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, Ridge\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "\n",
    "# Example dataset (replace with your own)\n",
    "X = pd.DataFrame({\n",
    "    \"age\": [23, 45, 31, 35, 40, 50, 29, 33],\n",
    "    \"salary\": [40000, 80000, 50000, 60000, 70000, 120000, 45000, 55000],\n",
    "    \"city\": [\"A\", \"B\", \"A\", \"C\", \"B\", \"C\", \"A\", \"C\"]\n",
    "})\n",
    "y = [200, 400, 250, 300, 350, 600, 220, 280]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Preprocessing\n",
    "numeric_features = [\"age\", \"salary\"]\n",
    "categorical_features = [\"city\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Feature selector + model\n",
    "feature_selector = RFECV(estimator=LinearRegression(), cv=3, scoring=\"r2\")\n",
    "\n",
    "# Final pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"feature_selection\", feature_selector),\n",
    "    (\"regressor\", LassoCV(cv=5, random_state=42))  # Could switch to Ridge, LinearRegression, etc.\n",
    "])\n",
    "\n",
    "# Fit pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R¬≤:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Save entire pipeline\n",
    "joblib.dump(pipeline, \"regression_pipeline.pkl\")\n",
    "\n",
    "# Load later\n",
    "loaded_pipeline = joblib.load(\"regression_pipeline.pkl\")\n",
    "print(\"Predictions (loaded):\", loaded_pipeline.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72c7806",
   "metadata": {},
   "source": [
    "üëâ So instead of saving just the model (which would forget how to encode or scale inputs), you save the pipeline, which knows how to:\n",
    "\n",
    "- Impute missing values\n",
    "- Encode categories\n",
    "- Scale numerics\n",
    "- Apply regression\n",
    "\n",
    "That makes it clean, consistent, and production-ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42e7c37",
   "metadata": {},
   "source": [
    "----\n",
    "| Feature                  | GridSearchCV                           | RidgeCV / LassoCV                      |\n",
    "|---------------------------|-----------------------------------------|----------------------------------------|\n",
    "| **Applies to**           | Any model in scikit-learn               | Only Ridge or Lasso regression         |\n",
    "| **Hyperparameters**      | Any (e.g., alpha, max_iter, solver, ‚Ä¶)  | Only alpha (regularization strength)   |\n",
    "| **Cross-validation**     | Yes (user-specified folds)              | Yes (built-in, default = 5 folds)      |\n",
    "| **Efficiency**           | Brute force search, can be slow         | Optimized solvers, very fast           |\n",
    "| **Ease of use**          | Flexible but more boilerplate code      | Very simple, just pass `alphas`        |\n",
    "| **Output**               | Best estimator + CV scores              | Best alpha + fitted model              |\n",
    "| **When to use**          | General hyperparameter tuning           | Regularization tuning (Ridge/Lasso)    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ec6e0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
